{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CentAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1ZH-w_xLJV4v-oCd-GtfYA9FWaloJVMqv\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR0gN7zhScJZ",
        "colab_type": "text"
      },
      "source": [
        "#**CentAI**\n",
        "By Justin Kim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUWc2wzrsZvs",
        "colab_type": "text"
      },
      "source": [
        "# DESCRIPTION\n",
        "This is an AI project to play a game.\n",
        "\n",
        "It uses Deep Reinforcement Learning (DQNAgent)\n",
        "\n",
        "https://keon.io/deep-q-learning/\n",
        "\n",
        "The game is Centakill, made by one of my friends, Kaiway Tang.\n",
        "\n",
        "Go check them out at: https://kaiway.itch.io/\n",
        "\n",
        "Or just the game at: https://kaiway.itch.io/centa\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6L6bjRuoHwQ",
        "colab_type": "text"
      },
      "source": [
        "NOTE:\n",
        "\n",
        "This only works if you run this on your local machine.\n",
        "\n",
        "You can do this by:\n",
        "\n",
        "1. copying the code into a local Python environment on your computer.\n",
        "2. connecting this notebook to a local runtime\n",
        "https://research.google.com/colaboratory/local-runtimes.html\n",
        "\n",
        "I would recommend option 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y41mkdsBpaEN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "For Option 1:\n",
        "\n",
        "I will assume you have basic Python knowledge and know how to use your Python editor (installing packages, creating new files) for this tutorial.\n",
        "\n",
        "1. Install the packages listed below\n",
        "2. Create a new file\n",
        "3. Copy code from each code box into the file\n",
        "4. Run the program\n",
        "5. If prompted, give the editor access to your system (so it can use press keys)\n",
        "6. If there are any questions, add a comment to my Youtube video and I'll try to help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FI_K2Jdpcmh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "For Option 2:\n",
        "\n",
        "It should be simpler, just downloading a couple things and enabling them, so I will only assume you know how to use your computer.\n",
        "\n",
        "1. Download Python https://www.python.org/downloads/\n",
        "\n",
        "2. Go to your terminal\n",
        "\n",
        "3. Do ```pip install jupyterlab```\n",
        "\n",
        "4. Do ```pip install jupyter_https_over_ws```\n",
        "\n",
        "5. Do ```jupyter serverextension enable --py jupyter_https_over_ws```\n",
        "\n",
        "6. Do \n",
        "```\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com'  --port=8888 --NotebookApp.port_retries=0\n",
        "```\n",
        "to start the notebook server. This should open a new tab in your web browser.\n",
        "\n",
        "7. Go back to this colab to the top right where it says RAM and Disk and click on the dropdown menu\n",
        "\n",
        "8. Press connect to local runtime and then press connect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CD0dERIlyYH",
        "colab_type": "text"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N24RhD4OlbY5",
        "colab_type": "text"
      },
      "source": [
        "installaton of various packages\n",
        "\n",
        "run only once at the beginning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWx6phKzlhRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy\n",
        "!pip install keras\n",
        "!pip install mss\n",
        "!pip install Pillow\n",
        "!pip install opencv-python\n",
        "!pip install pyautogui\n",
        "!pip install pytesseract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjNUFCql7Hpw",
        "colab_type": "text"
      },
      "source": [
        "imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqdMT12X7E_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from mss import mss\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import pyautogui\n",
        "import pytesseract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNWNe7Am4QSA",
        "colab_type": "text"
      },
      "source": [
        "This is DQNAgent. It was taken from https://keon.io/deep-q-learning/\n",
        "\n",
        "This is a class that contains the model.\n",
        "\n",
        "Tweak the values until the model learns better, these are just the default values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEE2d0OHv_PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=50000)\n",
        "        self.gamma = 0.95  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, min(batch_size, len(self.memory)))\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * \\\n",
        "                         np.amax(self.model.predict(next_state)[0])\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self):\n",
        "        self.model.save_weights('model.h5')\n",
        "\n",
        "    def load(self):\n",
        "        self.model.load_weights('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdPKffHmQ9xU",
        "colab_type": "text"
      },
      "source": [
        "Functions for the model to get input and interact with the computer.\n",
        "\n",
        "Run this cell once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpMBcYvLfSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def capture_ss():\n",
        "    img = sc.grab(monitor=sc.monitors[1])\n",
        "    return np.array(Image.frombytes('RGB', img.size, img.bgra, 'raw', 'BGRX'))\n",
        "\n",
        "\n",
        "def process_img(original_img):\n",
        "    processed_img = cv2.resize(original_img, scr_dim, interpolation=cv2.INTER_AREA)\n",
        "    processed_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
        "    return processed_img\n",
        "\n",
        "\n",
        "def shape_img(processed_img):\n",
        "    shaped_img = np.reshape(processed_img, [1, screen_size])\n",
        "    return shaped_img\n",
        "\n",
        "\n",
        "def act(action):\n",
        "    print(action)\n",
        "    if action > 4:\n",
        "        if action < 20:\n",
        "            if action % 4 == 0:\n",
        "                pyautogui.keyDown('W')\n",
        "            if action % 4 == 1:\n",
        "                pyautogui.keyDown('A')\n",
        "            if action % 4 == 2:\n",
        "                pyautogui.keyDown('S')\n",
        "            if action % 4 == 3:\n",
        "                pyautogui.keyDown('D')\n",
        "        else:\n",
        "            if action == 20 or action == 21 or action == 24 or action == 25 or action == 28 or action == 29:\n",
        "                pyautogui.keyDown('W')\n",
        "            if action == 20 or action == 22 or action == 24 or action == 26 or action == 28 or action == 30:\n",
        "                pyautogui.keyDown('A')\n",
        "            if action == 23 or action == 22 or action == 27 or action == 26 or action == 31 or action == 30:\n",
        "                pyautogui.keyDown('S')\n",
        "            if action == 23 or action == 21 or action == 27 or action == 25 or action == 31 or action == 29:\n",
        "                pyautogui.keyDown('D')\n",
        "    if action == 0 or (8 <= action <= 11) or (20 <= action <= 23):\n",
        "        pyautogui.keyDown('U')\n",
        "    if action == 1 or (12 <= action <= 15) or (24 <= action <= 27):\n",
        "        pyautogui.keyDown('I')\n",
        "    if action == 2:\n",
        "        pyautogui.keyDown('O')\n",
        "    if action == 3 or (16 <= action <= 19) or (28 <= action <= 31):\n",
        "        pyautogui.keyDown('P')\n",
        "\n",
        "\n",
        "def reset():\n",
        "    pyautogui.keyUp('W')\n",
        "    pyautogui.keyUp('A')\n",
        "    pyautogui.keyUp('S')\n",
        "    pyautogui.keyUp('D')\n",
        "    pyautogui.keyUp('U')\n",
        "    pyautogui.keyUp('I')\n",
        "    pyautogui.keyUp('O')\n",
        "    pyautogui.keyUp('P')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM4hKNfRRdgH",
        "colab_type": "text"
      },
      "source": [
        "The main function where all the AI happens.\n",
        "\n",
        "Run this once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbpxZCmhLfYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    agent = DQNAgent(screen_size, 4 + 4 + 12 + 12)\n",
        "    # agent.load()\n",
        "\n",
        "    death_flag = process_img(capture_ss())[death_loc[1], death_loc[0]]\n",
        "    print(\"starting\")\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    # repeat episode_num times\n",
        "    for e in range(episode_num):\n",
        "        pyautogui.keyDown('tab')\n",
        "        pyautogui.keyUp('tab')\n",
        "\n",
        "        score = 0\n",
        "        dead = False\n",
        "        last_scr = process_img(capture_ss())\n",
        "        last_scr_vector = shape_img(last_scr)\n",
        "        action = agent.act(last_scr_vector)\n",
        "        act(action)\n",
        "        last_time = time.time()\n",
        "\n",
        "        while not dead:\n",
        "            reset()\n",
        "            screen = capture_ss()\n",
        "            pro_scr = process_img(screen)\n",
        "            scr_vector = shape_img(pro_scr)\n",
        "\n",
        "            score_crop = screen[score_loc[1]:score_loc[1] + score_dim[1],\n",
        "                         score_loc[0]:score_loc[0] + score_dim[0]]\n",
        "            score_ocr = pytesseract.image_to_string(score_crop,\n",
        "                                                    config='--psm 10 --oem 3 -c tessedit_char_whitelist=0123456789')\n",
        "            if score_ocr != '':\n",
        "                score = max(score, int(score_ocr))\n",
        "\n",
        "            death_check = pro_scr[death_loc[1]][death_loc[0]]\n",
        "            if death_flag == death_check:\n",
        "                dead = True\n",
        "\n",
        "            reward = score\n",
        "\n",
        "            agent.remember(last_scr_vector, action, reward, scr_vector, dead)\n",
        "\n",
        "            if dead:\n",
        "                print('episode: {}/{}, score: {}'.format(e, episode_num, reward))\n",
        "                continue\n",
        "\n",
        "            print(time.time() - last_time)\n",
        "            last_time = time.time()\n",
        "            action = agent.act(scr_vector)\n",
        "            act(action)\n",
        "            last_scr = pro_scr\n",
        "            last_scr_vector = scr_vector\n",
        "\n",
        "        agent.replay(32)\n",
        "\n",
        "    agent.save()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCWKtXFRl9v",
        "colab_type": "text"
      },
      "source": [
        "This cell runs the main function. Run as many times as you want.\n",
        "\n",
        "You may need to change score_loc and score_dim to fit your computer size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKTD1dlsRw8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode_num = 10\n",
        "scr_dim = (200, 50)\n",
        "score_loc = (1085, 150)\n",
        "score_dim = (400, 150)\n",
        "death_loc = (int(scr_dim[0] / 2), int(scr_dim[1] / 2))\n",
        "screen_size = scr_dim[0] * scr_dim[1]\n",
        "\n",
        "sc = mss()\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
